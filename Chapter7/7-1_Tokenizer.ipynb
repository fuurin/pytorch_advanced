{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 形態素解析の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformerによるテキストデータのポジネガ分析  \n",
    "まずは文章を単語に分割，すなわち形態素解析を，以下のツールを使用して行う  \n",
    "- Janome\n",
    "- MeCab\n",
    "- NEologd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 必要なデータセットのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../datasets/ptca_datasets/chapter7\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec学習済みモデルをダウンロード\n",
    "\n",
    "東北大学 乾・岡崎研究室で[公開](http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/)されている[データ](http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/data/20170201.tar.bz2)をダウンロードして使用  \n",
    "  \n",
    "日本語Wikipediaにある単語を200次元のword2vecにしたデータセット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存先フォルダが存在しない場合は作成する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データのダウンロードを開始．15分ほどかかる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/data/20170201.tar.bz2\"\n",
    "save_path = os.path.join(data_dir, \"20170201.tar.bz2\")\n",
    "if not os.path.exists(save_path):\n",
    "    urllib.request.urlretrieve(url, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20170201.tar.bz2の解凍　5分ほどかかる  \n",
    "entity_vectorディレクトリと，その中にentity_vector.model.binができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open(save_path, 'r|bz2') # tarファイルを読み込み\n",
    "tar.extractall(data_dir)  # 解凍\n",
    "tar.close()\n",
    "os.remove(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDbデータセットをダウンロード\n",
    "\n",
    "映画評価データセットの[IMDbデータセット](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz)をダウンロード 30秒ほどかかる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "save_path = os.path.join(data_dir, \"aclImdb_v1.tar.gz\")\n",
    "if not os.path.exists(save_path):\n",
    "    urllib.request.urlretrieve(url, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aclImdb_v1.tar.gzの解凍 1分ほどかかる  \n",
    "aclImdbディレクトリができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open(save_path)\n",
    "tar.extractall(data_dir)\n",
    "tar.close()\n",
    "os.remove(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastTextの英語学習済みモデルをダウンロード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastTextの公式の英語学習済みモデル（650MB）をダウンロード 5分ほどかかる  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\"\n",
    "save_path = os.path.join(data_dir, \"wiki-news-300d-1M.vec.zip\")\n",
    "if not os.path.exists(save_path):\n",
    "    urllib.request.urlretrieve(url, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wiki-news-300d-1M.vec.zipを解凍  \n",
    "wiki-news-300d-1M.vecファイルができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = zipfile.ZipFile(save_path)\n",
    "file.extractall(data_dir)\n",
    "file.close()\n",
    "os.remove(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastTextの日本語学習済みモデルをダウンロード\n",
    "- [gdriveコマンド](https://takuya-1st.hatenablog.jp/entry/2016/07/06/034412)を使えるようにする  \n",
    "- data_dirへ行き，`gdrive download 0ByFQ96A4DgSPUm9wVWRLdm5qbmc`を実行\n",
    "    - tokenを求められるのでブラウザで取得\n",
    "- 次のセルのコードでzipを解凍\n",
    "    - model.vecが得られる\n",
    "\n",
    "参考：[Qiita 「いますぐ使える単語埋め込みベクトルのリスト」](https://qiita.com/Hironsan/items/8f7d35f0a36e0f99752c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(data_dir, \"vector_neologd.zip\")\n",
    "file = zipfile.ZipFile(save_path)\n",
    "file.extractall(data_dir)\n",
    "file.close()\n",
    "os.remove(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 著者によって用意されたファイル\n",
    "masterにある，chapter7_.../data内のファイル  \n",
    "- text_test.tsv\n",
    "- text_train.tsv\n",
    "- text_val.tsv\n",
    "\n",
    "をdata_dirへ移動しておく"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習における自然言語処理の流れ\n",
    "- コーパスの収集: 集めた文書データのこと  \n",
    "- クリーニング: HTMLやヘッダーといった文章ではないノイズ部分を削除すること\n",
    "- 正規化： 全角半角の統一，大文字小文字の統一，表記ゆれ(ex. 猫，ネコ，ねこ)の統一\n",
    "- 単語分割(形態素解析): 単語ごとに文章を区切る\n",
    "- 単語の基本形への変換: 原形，語幹への変換．ex. 走っ→走る\n",
    "- ストップワードの除去: 助詞など意味をあまり持たない単語を削除する\n",
    "- 単語の数値化: 単語にIDを振ったり，ベクトル化する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Janomeによる単語分割\n",
    "Janomeは日本語文章の単語分割(形態素解析)を行うパッケージ  \n",
    "Janomeをインストール\n",
    "``` bash\n",
    "$ pip insatll janome\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私\t名詞,代名詞,一般,*,*,*,私,ワタシ,ワタシ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "機械\t名詞,一般,*,*,*,*,機械,キカイ,キカイ\n",
      "学習\t名詞,サ変接続,*,*,*,*,学習,ガクシュウ,ガクシュー\n",
      "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "好き\t名詞,形容動詞語幹,*,*,*,*,好き,スキ,スキ\n",
      "です\t助動詞,*,*,*,特殊・デス,基本形,です,デス,デス\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "['私', 'は', '機械', '学習', 'が', '好き', 'です', '。']\n"
     ]
    }
   ],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "j_t = Tokenizer()\n",
    "\n",
    "text = '私は機械学習が好きです。'\n",
    "\n",
    "for token in j_t.tokenize(text):\n",
    "    print(token)\n",
    "\n",
    "# 分割結果だけが欲しい時には，wakati=Trueを与える\n",
    "print(j_t.tokenize(text, wakati=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MeCab+NEologdによる単語分割\n",
    "MeCabも単語分割(形態素解析)のライブラリ．  \n",
    "MeCabは，清語辞書であるNEologdと合わせて使用することができる．  \n",
    "  \n",
    "MeCabのインストール\n",
    "``` bash\n",
    "sudo apt install mecab libmecab-dev mecab-ipadic-utf8 -y\n",
    "pip install mecab-python3\n",
    "```\n",
    "\n",
    "NEologdのインストール\n",
    "``` bash\n",
    "git clone https://github.com/neologd/mecab-ipadic-neologd.git\n",
    "cd mecab-ipadic-neologd\n",
    "sudo bin/install-mecab-ipadic-neologd\n",
    "```\n",
    "インストールするか尋ねられ，yesと答えらインストール先ディレクトリが表示されるのでコピーしておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私\tワタシ\t私\t名詞-代名詞-一般\t\t\n",
      "は\tハ\tは\t助詞-係助詞\t\t\n",
      "機械\tキカイ\t機械\t名詞-一般\t\t\n",
      "学習\tガクシュウ\t学習\t名詞-サ変接続\t\t\n",
      "が\tガ\tが\t助詞-格助詞-一般\t\t\n",
      "好き\tスキ\t好き\t名詞-形容動詞語幹\t\t\n",
      "です\tデス\tです\t助動詞\t特殊・デス\t基本形\n",
      "。\t。\t。\t記号-句点\t\t\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "m_t = MeCab.Tagger('-Ochasen')\n",
    "\n",
    "text = '私は機械学習が好きです。'\n",
    "\n",
    "print(m_t.parse(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEologdを使うことで，「機械」と「学習」に分かれていた言葉が「機械学習」という単語になる  \n",
    "さっきコピーして置いたneologdのインストール先ディレクトリへのパスを使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私\tワタシ\t私\t名詞-代名詞-一般\t\t\n",
      "は\tハ\tは\t助詞-係助詞\t\t\n",
      "機械学習\tキカイガクシュウ\t機械学習\t名詞-固有名詞-一般\t\t\n",
      "が\tガ\tが\t助詞-格助詞-一般\t\t\n",
      "好き\tスキ\t好き\t名詞-形容動詞語幹\t\t\n",
      "です\tデス\tです\t助動詞\t特殊・デス\t基本形\n",
      "。\t。\t。\t記号-句点\t\t\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m_t = MeCab.Tagger('-Ochasen -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd')\n",
    "print(m_t.parse(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MeCab+NEologdによる文章の単語リストへの分割関数を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['私', 'は', '機械学習', 'が', '好き', 'です', '。']\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "def tokenizer_mecab(text, neologd_path='/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd'):\n",
    "    m_t = MeCab.Tagger(f'-Owakati -d {neologd_path}') # wakatiを使って品詞情報をなくす\n",
    "    text = m_t.parse(text)\n",
    "    return text.strip().split()\n",
    "\n",
    "text = '私は機械学習が好きです。'\n",
    "print(tokenizer_mecab(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
