{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchtextを用いたDataset, DataLoaderの実装\n",
    "torchtext: PyTorchの自然言語処理パッケージ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torchtextのインストール\n",
    "``` bash\n",
    "pip install torchtext\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用するデータ\n",
    "- text_train.tsv\n",
    "- text_val.tsv\n",
    "- text_test.tsv\n",
    "\n",
    "実際に学習をするわけではないので，上記3つはすべて同じで，4行しかない  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../datasets/ptca_datasets/chapter7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "王と王子と女王と姫と男性と女性がいました。\t0\n",
      "機械学習が好きです。\t1\n",
      "本章から自然言語処理に取り組みます。\t1\n",
      "本章では商品レビューの短い文章に対して、その文章がネガティブな評価をしている文章なのか、ポジティブな評価をしている文章なのか、2値のクラス分類する分類モデルを構築します。\t0\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(data_dir, \"text_train.tsv\"), encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章は特に意味がないが，0はネガティブ，1はポジティブのラベルになっている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理と単語分割の関数を実装\n",
    "まずはJanomeを使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "j_t = Tokenizer()\n",
    "\n",
    "def tokenizer_janome(text):\n",
    "    return j_t.tokenize(text, wakati=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前処理は\n",
    "- 今回行わないこと\n",
    "    - 全角・半角の統一\n",
    "    - 英語の小文字化\n",
    "    - 記号と数字の除去\n",
    "    - 特定文字を正規表現で置換\n",
    "- 今回行うこと\n",
    "    - [改行](https://ja.stackoverflow.com/questions/12897/%E6%94%B9%E8%A1%8C%E3%81%AE-n%E3%81%A8-r-n%E3%81%AE%E9%81%95%E3%81%84%E3%81%AF%E4%BD%95%E3%81%A7%E3%81%99%E3%81%8B)，半角スペース，全角スペースを削除\n",
    "    - 数字はすべて0にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    text = re.sub('\\r', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub(' ', '', text)\n",
    "    text = re.sub('　', '', text)\n",
    "    text = re.sub(r'[0-9 ０-９]', '0', text)\n",
    "    return text\n",
    "\n",
    "def tokenizer_with_preprocessing(text):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = tokenizer_janome(text)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['昨日', 'は', 'とても', '暑く', '，', '気温', 'が', '00', '度', 'も', 'あっ', 'た', '。']\n"
     ]
    }
   ],
   "source": [
    "text = \"昨日は とても暑く，気温が36度もあった。\"\n",
    "print(tokenizer_with_preprocessing(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文章データの読み込み\n",
    "torchtext.data.Fieldでフィールドごとのテキストの読み込み方を指定  \n",
    "引数には以下がある\n",
    "- sequential: データの長さが可変か？今回はTrue\n",
    "- tokenize: 前処理，単語分割をするための関数を指定\n",
    "- use_vocab: 単語をボキャブラリー(単語集)に追加するかを設定\n",
    "- lower: アルファベットを小文字に変換するかを指定\n",
    "- include_length: 文章の単語数のデータを保持するかの設定\n",
    "- batch_first: ミニバッチの次元を先頭に用意するかを指定\n",
    "- fix_length: すべての文章を，指定した長さと同じになるようpaddingする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "max_length = 25\n",
    "TEXT = torchtext.data.Field(\n",
    "    sequential=True,\n",
    "    tokenize=tokenizer_with_preprocessing,\n",
    "    use_vocab=True,\n",
    "    lower=True,\n",
    "    include_lengths=True,\n",
    "    batch_first=True,\n",
    "    fix_length=max_length\n",
    ")\n",
    "\n",
    "LABEL = torchtext.data.Field(\n",
    "    sequential=False,\n",
    "    use_vocab=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.TabularDatasetで文章データを読み込む  \n",
    "train,validation,testの3つのファイルを指定できる他，fieldsで各列に対して個別の処理を行うことができる．  \n",
    "出力される辞書のキーを一緒に指定できる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練データの数 4\n",
      "訓練データ[0] {'Text': ['王', 'と', '王子', 'と', '女王', 'と', '姫', 'と', '男性', 'と', '女性', 'が', 'い', 'まし', 'た', '。'], 'Label': '0'}\n",
      "訓練データ[1] {'Text': ['機械', '学習', 'が', '好き', 'です', '。'], 'Label': '1'}\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "    path=data_dir,\n",
    "    train='text_train.tsv',\n",
    "    validation='text_val.tsv',\n",
    "    test='text_test.tsv',\n",
    "    format='tsv',\n",
    "    fields=[('Text', TEXT), ('Label', LABEL)]\n",
    ")\n",
    "\n",
    "print('訓練データの数', len(train_ds))\n",
    "print('訓練データ[0]', vars(train_ds[0]))\n",
    "print('訓練データ[1]', vars(train_ds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なお,[vars](https://docs.python.org/ja/3/library/functions.html#vars)はオブジェクトを辞書として返すため，`__dict__`メソッドを呼び出す組み込み関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単語の数値化\n",
    "本節では，単語にIDを振ることで数値化する．  \n",
    "IDを振るには，ボキャブラリという単語の集まりを用意する．  \n",
    "全ての単語にIDを振るのではなく，扱う対象を絞ることが多い．  \n",
    "torchtext.data.Fieldインスタンスでボキャブラリを構築できる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'と': 5, '。': 4, 'の': 4, '文章': 4, 'な': 4, 'が': 3, '、': 3, 'を': 3, 'し': 3, '本章': 2, 'ます': 2, '評価': 2, 'て': 2, 'いる': 2, 'か': 2, '分類': 2, '王': 1, '王子': 1, '女王': 1, '姫': 1, '男性': 1, '女性': 1, 'い': 1, 'まし': 1, 'た': 1, '機械': 1, '学習': 1, '好き': 1, 'です': 1, 'から': 1, '自然': 1, '言語': 1, '処理': 1, 'に': 1, '取り組み': 1, 'で': 1, 'は': 1, '商品': 1, 'レビュー': 1, '短い': 1, 'に対して': 1, 'その': 1, 'ネガティブ': 1, 'ポジティブ': 1, '0': 1, '値': 1, 'クラス': 1, 'する': 1, 'モデル': 1, '構築': 1}) \n",
      "\n",
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7f9aad683da0>>, {'<unk>': 0, '<pad>': 1, 'と': 2, '。': 3, 'な': 4, 'の': 5, '文章': 6, '、': 7, 'が': 8, 'し': 9, 'を': 10, 'いる': 11, 'か': 12, 'て': 13, 'ます': 14, '分類': 15, '本章': 16, '評価': 17, '0': 18, 'い': 19, 'から': 20, 'する': 21, 'その': 22, 'た': 23, 'で': 24, 'です': 25, 'に': 26, 'に対して': 27, 'は': 28, 'まし': 29, 'クラス': 30, 'ネガティブ': 31, 'ポジティブ': 32, 'モデル': 33, 'レビュー': 34, '値': 35, '処理': 36, '取り組み': 37, '商品': 38, '女性': 39, '女王': 40, '好き': 41, '姫': 42, '学習': 43, '構築': 44, '機械': 45, '王': 46, '王子': 47, '男性': 48, '短い': 49, '自然': 50, '言語': 51})\n"
     ]
    }
   ],
   "source": [
    "# 訓練データからmin_freq以上の頻度の単語を使用してボキャブラリを構築\n",
    "TEXT.build_vocab(train_ds, min_freq=1)\n",
    "\n",
    "# メンバ変数にvocabが追加されている\n",
    "# 訓練データないの単語を頻度を出力\n",
    "print(TEXT.vocab.freqs, \"\\n\")\n",
    "\n",
    "# どの単語がどのIDに割り振られたのかを確認\n",
    "print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで，`<unk>`はunknownのことで，ボキャブラリにない単語を意味する．  \n",
    "`<pad>`は長さの統一のために使われるpaddingのこと"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaderの作成\n",
    "torchtext.data.Iteratorによってミニバッチサイズを指定したDataLoaderを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[46,  2, 47,  2, 40,  2, 42,  2, 48,  2, 39,  8, 19, 29, 23,  3,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1],\n",
      "        [45, 43,  8, 41, 25,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1]]), tensor([16,  6]))\n",
      "tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "train_dl = torchtext.data.Iterator(\n",
    "    train_ds, batch_size=2\n",
    ")\n",
    "\n",
    "val_dl = torchtext.data.Iterator(\n",
    "    val_ds, batch_size=2, train=False, sort=False\n",
    ")\n",
    "\n",
    "test_dl = torchtext.data.Iterator(\n",
    "    test_ds, batch_size=2, train=False, sort=False\n",
    ")\n",
    "\n",
    "batch = next(iter(val_dl))\n",
    "print(batch.Text)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語はIDに置き換えられており，25単語よりも短い文章は`<pad>＝1`が入っている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかし，単語はIDではなくベクトルで扱いたいので，その方法を次節で説明"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
