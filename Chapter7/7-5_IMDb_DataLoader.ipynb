{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDbのDataLoaderを実装\n",
    "本当は日本語の感情分析用データセットを使いたいが，最適なデータセットがないため，英語の映画レビューデータセットであるIMDb(Internet Movie Database)データセットを使用する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDbデータの用意\n",
    "torchtextにIMDbは用意されているが，今後も手持ちのデータで自然言語処理の実装が行えるよう，生のテキストデータを使うことにする  \n",
    "\n",
    "aclImddディレクトリ下には，test,trainディレクトリがあり，それぞれneg,posディレクトリ下にネガティブな評価の文(4/10以下)，ポジティブな評価の文(7/10以上)がtxt形式で入っている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの用意についての構造を以下にまとめておく  \n",
    "- テキストファイル(コーパス)を用意\n",
    "- tsv形式でテキストと特徴量(ラベル)をまとめて保存\n",
    "- 前処理関数tokenizer_with_preprocessingを定義\n",
    "    - preprocessing_text: 改行消したり，区切り文字消したりする\n",
    "    - tokenizer_punctuation: スペースで単語を区切り，token化(IDを割り振る)する\n",
    "- torchtext.data.Field(列)処理用関数を定義\n",
    "    - テキストにはtokenize=tokenizer_with_processingを指定\n",
    "- torchtext.data.TabularDataset.splitsで，作成したtsvファイルを処理\n",
    "    - Field処理用関数を通し，Text, Labelのkeyと対応するvalueを持つデータのリストを取得\n",
    "    - train_val_ds\n",
    "    - test_ds\n",
    "- train_val_dsをさらに8:2に分割\n",
    "    - train_ds\n",
    "    - val_ds\n",
    "- torchtext.vocab.Vectorsで英語版fasttextによる単語のベクトル表現モデルを取得\n",
    "- 上記で作成したtorchtext.data.Fieldのbuild_vocabメソッドでボキャブラリ(itos, stoi)を構築\n",
    "    - train_dsに含まれていて10回以上使われている単語のベクトル表現のみをenglish_fasttext_vectorsから取得\n",
    "    - TEXT.vocab.vectors[id]で単語のfasttextベクトル表現を取得できる\n",
    "- torchtext.data.Iteratorでhoge_dsからDataLoaderを作成\n",
    "    - DataLoaderはデータセットからIterableなバッチの集合を作成するので，あとはepochループ内で回すだけ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDbデータセットをtsv形式に変換\n",
    "IMDbデータを文章とラベル(0,1)からなるtsv形式に変換する  \n",
    "文章中のタブは消去する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, io\n",
    "data_dir = \"../../datasets/ptca_datasets/chapter7\"\n",
    "imdb_dir = os.path.join(data_dir, \"aclImdb\")\n",
    "cache_dir = os.path.join(data_dir, \"vector_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_label_tsv(input_dir, output_path, label, output_mode='a'):\n",
    "    with open(output_path, output_mode, encoding=\"utf-8\") as output_file:\n",
    "        for fname in glob.glob(os.path.join(input_dir, '*.txt')):\n",
    "            with io.open(fname, 'r', encoding=\"utf-8\") as input_file:\n",
    "                text = input_file.readline()\n",
    "                text = text.replace('\\t', \" \")\n",
    "                text = f\"{text}\\t{label}\\t\\n\"\n",
    "                output_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(imdb_dir, \"IMDb_train.tsv\")\n",
    "input_dir = os.path.join(imdb_dir, \"train\", \"pos\")\n",
    "make_text_label_tsv(input_dir, output_path, label=1, output_mode='w')\n",
    "input_dir = os.path.join(imdb_dir, \"train\", \"neg\")\n",
    "make_text_label_tsv(input_dir, output_path, label=0)\n",
    "\n",
    "output_path = os.path.join(imdb_dir, \"IMDb_test.tsv\")\n",
    "input_dir = os.path.join(imdb_dir, \"test\", \"pos\")\n",
    "make_text_label_tsv(input_dir, output_path, label=1, output_mode='w')\n",
    "input_dir = os.path.join(imdb_dir, \"test\", \"neg\")\n",
    "make_text_label_tsv(input_dir, output_path, label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imdb_dirにIMDb_train.tsvとIMDb_test.tsvが作成される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理と単語分割の関数を定義\n",
    "- 改行コード`<br/>`の削除\n",
    "- ピリオドとカンマ以外の記号をスペースに変えて除去\n",
    "- 単語分割は半角スペースで行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "区切り文字一覧： !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string, re\n",
    "print(\"区切り文字一覧：\", string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'cats', '.']\n"
     ]
    }
   ],
   "source": [
    "def preprocessing_text(text):\n",
    "    # 改行コードを消去\n",
    "    text = re.sub('<br />', '', text)\n",
    "    \n",
    "    # カンマ，ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        if (p == \".\") or (p ==\",\"):\n",
    "            # ピリオドとカンマの前後にはスペースを入れる\n",
    "            text = text.replace(p, f\" {p} \")\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenizer_punctuation(text):\n",
    "    # スペースで単語分割を行う\n",
    "    return text.strip().split()\n",
    "\n",
    "def tokenizer_with_preprocessing(text):\n",
    "    text = preprocessing_text(text)\n",
    "    tokens = tokenizer_punctuation(text)\n",
    "    return tokens\n",
    "\n",
    "print(tokenizer_with_preprocessing('I like cats.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaderの作成\n",
    "7.2節と大体同じだが，`init_token=\"<cls>\", eos_token=\"<eos>\"`を加える  \n",
    "普通init_tokenはbos(beggining of sentence)だが，あとでclassを意味していた方がいいことになるらしい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "max_length = 256\n",
    "TEXT = torchtext.data.Field(\n",
    "    sequential=True,\n",
    "    tokenize=tokenizer_with_preprocessing,\n",
    "    use_vocab=True,\n",
    "    lower=True, # 小文字化\n",
    "    include_lengths=True,\n",
    "    batch_first=True,\n",
    "    fix_length=max_length,\n",
    "    init_token=\"<cls>\",\n",
    "    eos_token=\"<eos>\"\n",
    ")\n",
    "\n",
    "LABEL = torchtext.data.Field(\n",
    "    sequential=False,\n",
    "    use_vocab=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練/検証データの数: 25000\n",
      "1つ目の訓練/検証データ: {'Text': ['wow', ',', 'alot', 'of', 'reviews', 'for', 'the', 'devils', 'experiment', 'are', 'here', '.', 'wonderful', '.', 'my', 'name', 'is', 'steve', 'and', 'i', 'run', 'unearthed', 'films', '.', 'we', 'just', 'started', 'releasing', 'the', 'guinea', 'pig', 'films', 'on', 'dvd', 'for', 'north', 'america', '.', 'now', 'before', 'you', 'ask', 'why', 'am', 'i', 'writing', 'a', 'review', 'instead', 'ask', 'why', 'some', 'people', 'bash', 'it', '.', 'i', 'm', 'writing', 'this', 'review', 'because', 'i', 'love', 'the', 'guinea', 'pig', 'films', '.', 'why', 'do', 'i', 'love', 'em', ',', 'it', 's', 'because', 'they', 'go', 'for', 'the', 'throat', 'and', 'they', 'don', 't', 'let', 'go', '.', 'i', 've', 'seen', 'it', 'all', '.', 'almost', 'every', 'horror', 'film', 'known', 'to', 'man', ',', 'argento', ',', 'fulci', ',', 'bava', ',', 'buttgereit', '.', 'from', 'every', 'underground', 'cult', 'sensation', 'to', 'every', 'hollywood', 'blockbuster', '.', 'i', 've', 'seen', 'it', 'all', 'and', 'the', 'films', 'that', 'have', 'stuck', 'in', 'my', 'head', 'over', 'the', 'years', 'was', 'definitely', 'the', 'guinea', 'pig', 'films', '.', 'why', 'because', 'it', 'doesn', 't', 'try', 'to', 'hide', 'the', 'reason', 'why', 'we', 'watch', 'horror', 'movies', 'in', 'the', '1st', 'place', '.', 'this', 'review', 'is', 'for', 'the', 'devils', 'experiment', '.', 'i', 'find', 'it', 'devoid', 'of', 'story', 'which', 'is', 'fine', 'by', 'me', '.', 'why', 'do', 'i', 'watch', 'horror', 'films', 'so', 'i', 'can', 'see', 'blood', 'and', 'gore', 'and', 'the', 'torture', 'of', 'people', '.', 'the', 'devils', 'experiment', 'not', 'only', 'delivers', 'but', 'that', 's', 'all', 'it', 'is', '.', 'pure', 'unadulterated', 'violence', '.', 'yeah', 'i', 'like', 'a', 'story', 'but', 'sometimes', 'i', 'just', 'want', 'the', 'gore', 'and', 'the', 'devils', 'experiment', 'delivers', 'ten', 'fold', '.', 'why', 'do', 'people', 'bash', 'it', '.', 'cause', 'they', 'like', 'a', 'story', ',', 'so', 'that', 'the', 'torture', 'and', 'death', 'of', 'a', 'person', 'can', 'be', 'hidden', 'behind', 'a', 'story', '.', 'it', 'make', 'em', 'feel', 'better', 'about', 'themselves', '.', 'we', 'all', 'want', 'blood', 'and', 'gore', '.', 'it', 's', 'just', 'really', 'hard', 'to', 'justify', 'it', 'if', 'it', 's', 'not', 'wrapped', 'around', 'a', 'story', '.', 'the', 'guinea', 'pig', 'films', 'have', 'a', 'historical', 'meaning', 'to', 'them', 'and', 'they', 'have', 'created', 'a', 'definitive', 'splash', 'whenever', 'they', 'have', 'been', 'released', '.', 'i', 'm', 'thrilled', 'to', 'be', 'able', 'to', 'release', 'one', 'of', 'the', 'most', 'famous', 'horror', 'series', 'in', 'the', 'world', '.', 'maybe', 'i', 'shouldn', 't', 'have', 'written', 'this', 'review', 'but', 'then', 'again', 'maybe', 'i', 'should', '.', 'my', 'view', 'is', 'biased', 'cause', 'were', 'releasing', 'them', 'but', 'then', 'again', 'it', 's', 'not', '.', 'i', 've', 'always', 'told', 'people', 'to', 'find', 'them', 'and', 'to', 'watch', 'them', 'way', 'before', 'i', 'started', 'unearthed', 'films', '.', 'sure', 'it', 's', 'exploitive', 'and', 'over', 'the', 'top', 'but', 'isn', 't', 'that', 'why', 'we', 'watch', 'horror', 'films', 'in', 'the', '1st', 'place', '.', 'the', 'devils', 'experiment', 'is', 'not', 'for', 'everybody', '.', 'it', 's', 'for', 'thrill', 'seekers', 'and', 'gorehounds', 'only', '.', 'if', 'you', 'think', 'jason', 'movies', 'and', 'freddy', 'krueger', 'movies', 'are', 'awesome', 'then', 'stick', 'to', 'those', '.', 'but', 'if', 'your', 'on', 'the', 'next', 'level', 'and', 'have', 'seen', 'it', 'all', 'then', 'the', 'devils', 'experiment', 'is', 'for', 'you', '.', 'there', 'is', 'a', 'reason', 'why', 'they', 'haven', 't', 'been', 'released', 'for', 'over', '17', 'years', '.', 'they', 'are', 'wrong', ',', 'disgusting', 'and', 'down', 'right', 'freaky', 'and', 'not', 'something', 'to', 'watch', 'with', 'your', 'mom', ',', 'unless', 'she', 'is', 'totally', 'cool', '.', 'good', 'luck', ',', 'enjoy', 'and', 'never', 'stop', 'living', 'your', 'life', '.'], 'Label': '1'}\n"
     ]
    }
   ],
   "source": [
    "train_val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "    path=imdb_dir,\n",
    "    train='IMDb_train.tsv',\n",
    "    test='IMDb_test.tsv',\n",
    "    format='tsv',\n",
    "    fields=[('Text', TEXT), ('Label', LABEL)]\n",
    ")\n",
    "\n",
    "print('訓練/検証データの数:', len(train_val_ds))\n",
    "print('1つ目の訓練/検証データ:', vars(train_val_ds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらにtrain_val_dsを訓練データとvalidationデータに8:2で分ける  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練データの数: 20000\n",
      "検証データの数: 5000\n",
      "1つ目の訓練データ: {'Text': ['i', 'think', 'micheal', 'ironsides', 'acting', 'career', 'must', 'be', 'over', ',', 'if', 'he', 'has', 'to', 'star', 'in', 'this', 'sort', 'of', 'low', 'budge', 'crap', '.', 'surely', 'he', 'could', 'do', 'better', 'than', 'waste', 'his', 'time', 'in', 'this', 'rubbish', '.', 'this', 'movie', 'could', 'be', 'far', 'better', ',', 'if', 'it', 'had', 'a', 'good', 'budget', ',', 'but', 'it', 'shows', 'repeatedly', 'through', 'out', 'the', 'movie', '.', 'there', 'is', 'one', 'scene', 'at', 'a', 'outpost', ',', 'which', 'looks', 'like', ',', 'its', 'outside', 'the', 'front', 'of', 'a', 'railway', 'station', ',', 'and', 'i', 'bet', 'it', 'was', '.', 'there', 'is', 'one', 'scene', 'which', 'made', 'give', 'this', 'movie', 'a', '3', ',', 'and', 'it', 'shows', 'the', 'space', 'craft', 'landing', 'and', 'taking', 'off', 'over', 'a', 'lake', ',', 'surrounded', 'by', 'forests', '.', 'this', 'was', 'well', 'done', ',', 'but', 'the', 'rest', 'of', 'the', 'movie', ',', 'forget', 'it', '.', 'there', 'is', 'another', 'scene', ',', 'which', 'looks', 'like', 'a', 'engineering', 'plant', ',', 'which', 'i', 'bet', 'it', ',', 'and', 'does', 'not', 'look', 'like', 'a', 'space', 'outpost', 'as', 'the', 'character', 'say', 'it', 'is', '.', 'this', 'movie', 'is', 'stupid', ',', 'has', 'a', 'serious', 'low', 'budget', ',', 'makes', 'no', 'sense', 'and', 'god', 'help', 'micheal', 'ironsides', '.'], 'Label': '0'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "train_ds, val_ds = train_val_ds.split(split_ratio=0.8, random_state=random.seed(1234))\n",
    "print(\"訓練データの数:\", len(train_ds))\n",
    "print(\"検証データの数:\", len(val_ds))\n",
    "print(\"1つ目の訓練データ:\", vars(train_ds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ボキャブラリーを作成\n",
    "分散表現には英語版のfastTextであるwiki-news-300d-1M.vecを使用する  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/999994 [00:00<?, ?it/s]Skipping token b'999994' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|█████████▉| 999586/999994 [01:31<00:00, 11105.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1単語を表現する次元数: 300\n",
      "単語数: 999994\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import Vectors\n",
    "\n",
    "english_fasttext_vectors = Vectors(\n",
    "    name=os.path.join(data_dir, \"wiki-news-300d-1M.vec\"),\n",
    "    cache=cache_dir\n",
    ")\n",
    "\n",
    "print(\"1単語を表現する次元数:\", english_fasttext_vectors.dim)\n",
    "print(\"単語数:\", len(english_fasttext_vectors.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17901, 300])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0786,  0.0097,  0.0023,  ...,  0.0901,  0.0283,  0.0346],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0147, -0.0415,  0.0006,  ...,  0.0387, -0.0181, -0.0128]])\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_ds, vectors=english_fasttext_vectors, min_freq=10)\n",
    "\n",
    "print(TEXT.vocab.vectors.shape)\n",
    "\n",
    "print(TEXT.vocab.vectors)\n",
    "\n",
    "# print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後にDataLoaderを作成する  \n",
    "データセットではベクトル表現ではなく単語IDであり，大量のメモリ消費を防いでいる．  \n",
    "学習時はモデル側でIDに応じてベクトル表現を取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[  2, 416,  36,  ..., 353,   5,   3],\n",
      "        [  2,   4, 114,  ..., 394,  13,   3],\n",
      "        [  2,  14, 246,  ...,   1,   1,   1],\n",
      "        ...,\n",
      "        [  2,  15,  11,  ...,   1,   1,   1],\n",
      "        [  2,  15,  24,  ...,   1,   1,   1],\n",
      "        [  2,  41, 923,  ...,  16,   4,   3]]), tensor([256, 256,  96, 256, 149, 135, 256, 256, 145, 256, 192, 256, 256, 164,\n",
      "        239, 256, 155, 156, 256, 178, 151, 102, 217, 256]))\n",
      "tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 999586/999994 [01:50<00:00, 11105.80it/s]"
     ]
    }
   ],
   "source": [
    "train_dl = torchtext.data.Iterator(train_ds, batch_size=24, train=True)\n",
    "val_dl = torchtext.data.Iterator(val_ds, batch_size=24, train=False, sort=False)\n",
    "test_dl = torchtext.data.Iterator(test_ds, batch_size=24, train=False, sort=False)\n",
    "\n",
    "batch = next(iter(val_dl))\n",
    "print(batch.Text)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上により，IMDbの各DataLoaderと訓練データの単語を使用したボキャブラリーの分散ベクトルを用意できた．  \n",
    "以上の内容はutils/dataloader.pyに用意し，次節で使用できるようにする  \n",
    "次節では以上のDataLoaderと単語ベクトルを使用し，文章のネガポジ感情分析を実現するTransformerを実装する"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
