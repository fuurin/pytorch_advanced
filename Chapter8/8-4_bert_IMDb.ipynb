{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTの学習・推論・判定根拠可視化\n",
    "IMDbのポジネガ判定をBERTでやってみる  \n",
    "分類タスク用のアダプターモジュールを追加してファインチューニングする  \n",
    "また，Self-Attentionの重みを可視化し，推論で重要となる単語をハイライト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDbデータを読み込み，DataLoaderを作成\n",
    "7章と異なる点があるのでここで再実装\n",
    "- Bert用のWordPieceを用いてサブワードに対応したTokenizerを使用\n",
    "- 訓練データに含まれている単語ではなく，BERTが持つ全単語を使用\n",
    "    - BERTEmbeddingモジュールでは全単語を使用する\n",
    "    - bert-base-uncased-vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time, tqdm, string, random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from utils.bert import BertTokenizer, load_vocab\n",
    "from IPython.display import HTML\n",
    "\n",
    "data_dir = \"../../datasets/ptca_datasets/chapter8\"\n",
    "imdb_dir = os.path.join(data_dir, \"aclImdb\")\n",
    "vocab_dir = os.path.join(data_dir, \"vocab\")\n",
    "weights_dir = os.path.join(data_dir, \"weights\")\n",
    "vocab_save_path=os.path.join(vocab_dir, \"bert-base-uncased-vocab.txt\")\n",
    "weights_save_path = os.path.join(weights_dir, \"pytorch_model.bin\")\n",
    "config_save_path = os.path.join(weights_dir, \"bert_config.json\")\n",
    "model_save_path = os.path.join(data_dir, \"bert_fine_tuning_IMDb.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDbの前処理(7章と同じ)\n",
    "def preprocessing_text(text):\n",
    "    text = re.sub('<br />', '', text)\n",
    "    \n",
    "    # カンマ，ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        if (p == \".\") or (p ==\",\"):\n",
    "            # ピリオドとカンマの前後にはスペースを入れる\n",
    "            text = text.replace(p, f\" {p} \")\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 違うのはTokenizerがサブワード対応＆BERTのボキャブラリを使用していること\n",
    "tokenizer_bert = BertTokenizer(vocab_save_path)\n",
    "\n",
    "def tokenizer_with_preprocessing(text, tokenizer=tokenizer_bert.tokenize):\n",
    "    text = preprocessing_text(text)\n",
    "    return tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを読み込んだ時の処理をTEXT，LABELとして用意  \n",
    "max_length=256で，BERTに入力するとき`<PAD>`を入れて512単語にする  \n",
    "(SEPで2文に分割することはしない)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "TEXT = torchtext.data.Field(\n",
    "    sequential=True,\n",
    "    tokenize=tokenizer_with_preprocessing,\n",
    "    use_vocab=True,\n",
    "    lower=True,\n",
    "    include_lengths=True,\n",
    "    batch_first=True,\n",
    "    fix_length=max_length,\n",
    "    init_token=\"[CLS]\",\n",
    "    eos_token=\"[SEP]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    unk_token=\"[UNK]\"\n",
    ")\n",
    "\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDbを整形したtsvファイルを読み込み，Datasetにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "    path=imdb_dir,\n",
    "    train=\"IMDb_train.tsv\",\n",
    "    test=\"IMDb_test.tsv\",\n",
    "    format='tsv',\n",
    "    fields=[('Text', TEXT), ('Label', LABEL)]\n",
    ")\n",
    "\n",
    "train_ds, val_ds = train_val_ds.split(split_ratio=0.8, random_state=random.seed(1234))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  単語->ID, ID->単語\n",
    "vocab_bert, ids_to_tokens_bert = load_vocab(vocab_save_path)\n",
    "\n",
    "# TEXT.vocabを生成するため適当なデータでvocabを作ってからstoiを上書き\n",
    "# もう少しいい方法があるのでは？\n",
    "TEXT.build_vocab(train_ds, min_freq=1)\n",
    "TEXT.vocab.stoi = vocab_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXTに単語->IDであるボキャブラリを用意できたので，DataLoaderを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dl = torchtext.data.Iterator(\n",
    "    train_ds, batch_size=batch_size, train=True\n",
    ")\n",
    "\n",
    "val_dl = torchtext.data.Iterator(\n",
    "    val_ds, batch_size=batch_size, train=False, sort=False\n",
    ")\n",
    "\n",
    "test_dl = torchtext.data.Iterator(\n",
    "    test_ds, batch_size=batch_size, train=False, sort=False\n",
    ")\n",
    "\n",
    "dataloaders_dict = {\"train\": train_dl, \"val\": val_dl}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[  101,  5791,  2012,  ...,  1012,  1999,   102],\n",
      "        [  101,  1996,  5436,  ...,  1996,  2785,   102],\n",
      "        [  101,  1045,  2572,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2045,  2024,  ..., 11790,  2027,   102],\n",
      "        [  101,  5745,  2466,  ...,     0,     0,     0],\n",
      "        [  101,  2073,  1996,  ...,  4237,  1012,   102]]), tensor([256, 256,  99, 256, 154, 137, 256, 256, 148, 256, 194, 256, 256, 170,\n",
      "        246, 256, 159, 165, 256, 179, 156, 113, 228, 256, 256, 256, 256, 256,\n",
      "        256, 256, 107, 256]))\n",
      "tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
      "        0, 1, 1, 1, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_dl))\n",
    "print(batch.Text)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ミニバッチの1文目を確認してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'the', 'plot', 'of', 'this', 'film', 'might', 'not', 'be', 'extraordinary', ',', 'but', 'what', 'makes', 'the', 'film', 'really', 'special', ',', 'are', 'its', 'characters', 'and', 'the', 'actors', 'who', 'play', 'them', 'of', 'course', '.', 'i', 'won', 't', 'go', 'into', 'the', 'details', 'of', 'the', 'plot', 'of', 'the', 'movie', ',', 'but', 'i', 'would', 'certainly', 'like', 'to', 'say', 'this', 'this', 'film', 'is', 'not', 'just', 'for', 'everyone', 'the', 'film', 'is', 'really', 'witty', 'and', 'you', 'need', 'to', 'be', 'equally', 'clever', 'to', 'get', 'all', 'the', 'satire', '.', 'if', 'you', 're', 'not', 'alert', 'even', 'for', 'a', 'second', ',', 'you', 'll', 'probably', 'end', 'up', 'missing', 'one', 'of', 'the', 'subtle', 'points', '.', 'the', 'movie', 'is', 'full', 'of', 'such', 'seemingly', 'trivial', 'but', 'witty', 'stuff', 'like', 'the', 'announcements', 'going', 'on', 'in', 'the', 'background', 'at', 'tu', '##ra', '##qi', '##stan', ',', 'the', 'advertisements', 'on', 'the', 'tanker', '##s', 'which', 'i', 'almost', 'missed', 'and', 'it', 'are', 'these', 'that', 'make', 'the', 'movie', 'hilarious', 'throughout', '.', 'coming', 'to', 'the', 'actors', ',', 'john', 'cu', '##sack', 'has', 'played', 'his', 'multi', 'face', '##ted', 'role', 'very', 'efficiently', 'what', 'with', 'him', 'being', 'the', 'co', 'writer', 'and', 'the', 'producer', 'too', 'and', 'he', 'plays', 'his', 'character', 'ha', '##user', ',', 'the', 'killer', 'with', 'a', 'heart', 'exquisite', '##ly', '.', 'cu', '##sack', 's', 'done', 'a', 'similar', 'kind', 'of', 'role', 'before', 'in', 'gross', '##e', 'pointe', 'blank', ',', 'but', 'his', 'comic', 'disposition', 'in', 'the', 'movie', 'is', 'simply', 'superb', '.', 'however', 'the', 'actress', 'who', 'steals', 'all', 'the', 'show', 'is', 'hilary', 'duff', 'i', 'have', 'always', 'been', 'a', 'huge', 'fan', 'of', 'ms', '.', 'duff', '.', 'but', 'to', 'be', 'honest', 'i', 'was', 'a', 'bit', 'disappointed', 'when', 'i', 'heard', 'about', 'the', 'kind', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text_minibatch_1 = batch.Text[0][1].numpy()\n",
    "text = tokenizer_bert.convert_ids_to_tokens(text_minibatch_1)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`##word`は前に繋がる単語に付随するサブワードを意味する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感情分析用のBERTモデルを構築\n",
    "- 学習済みパラメータをロード\n",
    "- ポジネガ分類用モジュールを取り付ける\n",
    "- 感情分析を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bert import get_config, BertModel, set_learned_params\n",
    "\n",
    "config = get_config(config_save_path)\n",
    "net_bert = BertModel(config)\n",
    "net_bert = set_learned_params(net_bert, weights_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERTの基礎部分にポジネガ分類のための全結合層1つによるアダプタを取り付け  \n",
    "BERTの先頭単語の特徴量はNSPにより入力文章全体の特徴を反映している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForIMDb(nn.Module):\n",
    "    def __init__(self, net_bert):\n",
    "        super(BertForIMDb, self).__init__()\n",
    "        \n",
    "        self.bert = net_bert\n",
    "        self.cls = nn.Linear(in_features=768, out_fetures=2)\n",
    "        \n",
    "        # ポジネガ分類モジュールだけ重みの正規分布初期化を行う\n",
    "        nn.init.normal_(self.cls.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls.bias, 0)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None,\n",
    "                output_all_encoded_layers=False, attention_show_flg=False):\n",
    "        \n",
    "        if attention_show_flg:\n",
    "            encoded_layers, pooled_output, attention_probs = self.bert(\n",
    "                input_ids, token_type_ids, attention_mask,\n",
    "                output_all_encoded_layers, attention_show_flg\n",
    "            )\n",
    "        else:\n",
    "            encoded_layers, pooled_output = self.bert(\n",
    "                input_ids, token_type_ids, attention_mask,\n",
    "                output_all_encoded_layers, attention_show_flg\n",
    "            )\n",
    "        \n",
    "        # 入力文章の最初の単語の部分を使用してポジネガ分類を行う\n",
    "        vec_0 = encoded_layers[:, 0, :]\n",
    "        vec_0 = vec_0.view(-1, 768)\n",
    "        out = self.cls(vec_0)\n",
    "        \n",
    "        if attention_show_flg:\n",
    "            return out, attention_probs\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bertのファインチューニングに向けた設定\n",
    "12段全てをファインチューニングすると時間がかかるので，最終段のみ訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, param in net.bert.encoder.layer[-1].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for name, param in net.cls.named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習・検証を実施\n",
    "attention_maskの`<PAD>`されている部分には意味がないとだいたい学習できているため，attention_maskをNoneにして全てにSelf-Attentionをかけてしまう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(device, net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    net.to(device)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == \"train\":\n",
    "                net.train()\n",
    "            else:\n",
    "                net.eval()\n",
    "            \n",
    "            epoch_loss = 0.0\n",
    "            epoch_corrects = 0\n",
    "            iteration = 1\n",
    "            \n",
    "            t_epoch_start = time.time()\n",
    "            t_iter_start = time.time()\n",
    "            \n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                inputs = batch.Text[0].to(device)\n",
    "                labels = batch.Label.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        if (iteration % 10 == 0):\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            acc = (torch.sum(preds == labels.data))\n",
    "                            acc = acc.double() / batch_size\n",
    "                            print(f\"Iteration {iteration} || Loss: {loss.item():.4f} || Acc: {acc:.4f} || {duration:.4f} sec\")\n",
    "                            t_iter_start = time.time()\n",
    "                \n",
    "                iteration += 1\n",
    "                epoch_loss += loss.item() * batch_size\n",
    "                epoch_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "            t_epoch_finish = time.time()\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
    "            \n",
    "            print(f\"Epoch {epoch}/{num_epochs} | {phase:^5} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}\")\n",
    "            t_epoch_start(time.time())\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "net = BertForIMDb(net_bert)\n",
    "print('ネットワーク設定完了')\n",
    "\n",
    "# ハイパーパラメータはBERTの論文で推奨されている値を使用\n",
    "optimizer = optim.Adam([\n",
    "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
    "    {'params': net.cls.parameters(), 'lr': 5e-5}\n",
    "], betas = (0.9, 0.999))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "net_trained = train_model(\n",
    "    device, \n",
    "    net, \n",
    "    dataloaders_dict, \n",
    "    criterion, \n",
    "    optimizer,\n",
    "    num_epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータを保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net_trained.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(config_save_path)\n",
    "net_bert = BertModel(config)\n",
    "net = BertForIMDb(net_bert)\n",
    "net.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータで正解率を確認してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "net_trained.eval()\n",
    "net_trained.to(device)\n",
    "\n",
    "epoch_corrects = 0\n",
    "for batch in tqdm(test_dl):\n",
    "    inputs = batch.Text[0].to(device)\n",
    "    labels = batch.Label.to(device)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = net_trained(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    epoch_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "epoch_acc = epoch_corrects.double() / len(test_dl.dataset)\n",
    "print(f\"テストデータ{len(test_dl.dataset)}個での正解率： {epoch_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解率が90%を超えた  \n",
    "7章では80%だったので，大きく正答率が向上したと言える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attentionの可視化\n",
    "Self-Attentionの重みを可視化し，推論  \n",
    "テストデータの最初の64文章を推論してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "test_dl = torchtext.data.Iterator(\n",
    "    test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "batch = next(iter(test_dl))\n",
    "inputs = batch.Text[0].to(device)\n",
    "labels = batch.Label.to(device)\n",
    "\n",
    "outputs, attention_probs = net_trained(inputs, attention_show_flg=True)\n",
    "_, preds = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章をAttentionの重みに応じて色つけするHTMLを作成  \n",
    "7章とほぼ同じだが，次の点が異なる  \n",
    "- 前回は初段と終段の2つだったが，今回は終段のAttentionのみ扱う\n",
    "- multi-headedな出力それぞれについて可視化の結果を確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight(word, attn):\n",
    "    ''' Attentionの値が大きいと文字の背景が濃い赤になるHTMLを作成 '''\n",
    "    # 16進数2文字で3つの値を出力\n",
    "    color = '#%02X%02X%02X' % (255, int(255*(1-attn)), int(255*(1-attn)))\n",
    "    html = f' <span style=\"background-color: {color}\">{word}</span>'\n",
    "    return html\n",
    "\n",
    "def highlight_sentence(sentence, attns):\n",
    "    ''' 各単語をAttentionの値に応じてハイライト, SEPが出てきたらそこで終わる '''\n",
    "    html = \"\"\n",
    "    for word, attn in zip(sentence, attns):\n",
    "        word = [word.numpy().tolist()]\n",
    "        word = tokenizer_bert.convert_ids_to_tokens(word)[0]\n",
    "        if word == \"[SEP]\":\n",
    "            break\n",
    "        html += highlight(word, attn)\n",
    "    return html\n",
    "\n",
    "def mk_html(index, batch, preds, normalized_weights, TEXT):\n",
    "    # indexの結果を抽出\n",
    "    sentence = batch.Text[0][index]\n",
    "    label = batch.Label[index]\n",
    "    pred = preds[index]\n",
    "    \n",
    "    # ラベルと予測結果を文字に置き換え\n",
    "    label_str = \"Negative\" if label == 0 else \"Positive\"\n",
    "    pred_str = \"Negative\" if pred == 0 else \"Positive\"\n",
    "    \n",
    "    # HTMLの作成\n",
    "    html = f\"正解ラベル:{label_str}<br>推論ラベル:{pred_str}<br><br>\"\n",
    "    \n",
    "    # 12個のMulti-Head Self-Attentionそれぞれの重みを可視化\n",
    "    for i in range(12):\n",
    "        \n",
    "        # indexのAttentionを抽出し，規格化\n",
    "        # 0単語目のi番目のMulti-Head Attentionを取り出す\n",
    "        attens = normalized_weights[index, i, 0, :]\n",
    "        attens /= attens.max()\n",
    "        \n",
    "        html += f\"[BERTのAttention {i+1:^2} を可視化]<br>\"\n",
    "        html += highlight_sentence(sentence, attens)\n",
    "        html += \"<br><br>\"\n",
    "    \n",
    "    # 全Attentionの重みの平均を可視化\n",
    "    all_attens = attens * 0 # zeros like\n",
    "    for i in range(12):\n",
    "        all_attens += normalized_weights[index, i, 0, :]\n",
    "    all_attens /= all_attens.max()\n",
    "    \n",
    "    html += '[BERTのAttention全体の平均を可視化]<br>'\n",
    "    html += highlight_sentence(sentence, all_attens)\n",
    "    html += \"<br><br>\"\n",
    "    \n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "うまく判定できている場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 3 # 可視化対象文章のID\n",
    "html = mk_html(index, batch, preds, attention_probs, TEXT)\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7章でうまく判定できていなかった文章の場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0 # 可視化対象文章のID\n",
    "html = mk_html(index, batch, preds, attention_probs, TEXT)\n",
    "HTML(html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
