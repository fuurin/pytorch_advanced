{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTの学習・推論・判定根拠可視化\n",
    "IMDbのポジネガ判定をBERTでやってみる  \n",
    "分類タスク用のアダプターモジュールを追加してファインチューニングする  \n",
    "また，Self-Attentionの重みを可視化し，推論で重要となる単語をハイライト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDbデータを読み込み，DataLoaderを作成\n",
    "7章と異なる点があるのでここで再実装\n",
    "- Bert用のWordPieceを用いてサブワードに対応したTokenizerを使用\n",
    "- 訓練データに含まれている単語ではなく，BERTが持つ全単語を使用\n",
    "    - BERTEmbeddingモジュールでは全単語を使用する\n",
    "    - bert-base-uncased-vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, string\n",
    "from utils.bert import BertTokenizer\n",
    "\n",
    "data_dir = \"../../datasets/ptca_datasets/chapter8\"\n",
    "imdb_dir = os.path.join(data_dir, \"aclImdb\")\n",
    "vocab_dir = os.path.join(data_dir, \"vocab\")\n",
    "weights_dir = os.path.join(data_dir, \"weights\")\n",
    "vocab_save_path=os.path.join(vocab_dir, \"bert-base-uncased-vocab.txt\")\n",
    "weights_save_path = os.path.join(weights_dir, \"pytorch_model.bin\")\n",
    "config_save_path = os.path.join(weights_dir, \"bert_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDbの前処理(7章と同じ)\n",
    "def preprocessing_text(text):\n",
    "    text = re.sub('<br />', '', text)\n",
    "    \n",
    "    # カンマ，ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        if (p == \".\") or (p ==\",\"):\n",
    "            # ピリオドとカンマの前後にはスペースを入れる\n",
    "            text = text.replace(p, f\" {p} \")\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 違うのはTokenizerがサブワード対応＆BERTのボキャブラリを使用していること\n",
    "tokenizer_bert = BertTokenizer(vocab_save_path)\n",
    "\n",
    "def tokenizer_with_preprocessing(text, tokenizer=tokenizer_bert.tokenize):\n",
    "    text = preprocessing_text(text)\n",
    "    return tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを読み込んだ時の処理をTEXT，LABELとして用意  \n",
    "max_length=256で，BERTに入力するとき`<PAD>`を入れて512単語にする  \n",
    "(SEPで2文に分割することはしない)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "max_length = 256\n",
    "\n",
    "TEXT = torchtext.data.Field(\n",
    "    sequential=True,\n",
    "    tokenize=tokenizer_with_preprocessing,\n",
    "    use_vocab=True,\n",
    "    lower=True,\n",
    "    include_lengths=True,\n",
    "    batch_first=True,\n",
    "    fix_length=max_length,\n",
    "    init_token=\"[CLS]\",\n",
    "    eos_token=\"[SEP]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    unk_token=\"[UNK]\"\n",
    ")\n",
    "\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDbを整形したtsvファイルを読み込み，Datasetにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "    path=imdb_dir,\n",
    "    train=\"IMDb_train.tsv\",\n",
    "    test=\"IMDb_test.tsv\",\n",
    "    format='tsv',\n",
    "    fields=[('Text', TEXT), ('Label', LABEL)]\n",
    ")\n",
    "\n",
    "train_ds, val_ds = train_val_ds.split(split_ratio=0.8, random_state=random.seed(1234))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
